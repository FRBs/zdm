{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "- Used to visualise HDF5 files from MCMC analysis\n",
    "- Developed to handle output files from MCMC.py and MCMC2.py\n",
    "- Produces plots for walkers\n",
    "- Produces corner plot\n",
    "- Produces more detailed analysis for the best fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import emcee\n",
    "import json\n",
    "\n",
    "from zdm import survey\n",
    "from zdm import cosmology as cos\n",
    "from zdm import loading as loading\n",
    "import zdm.misc_functions as mf\n",
    "import zdm.iteration as it\n",
    "from zdm import parameters\n",
    "from zdm.MCMC import calc_log_posterior\n",
    "from astropy.cosmology import Planck18\n",
    "\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files\n",
    "\n",
    "- labels = list of parameters (in order)\n",
    "- filenames = list of .h5 files to use (without .h5 extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [\"logF\", \"sfr_n\", \"alpha\", \"lmean\", \"lsigma\", \"lEmax\", \"lEmin\", \"gamma\", \"DMhalo\", \"H0\"]\n",
    "# labels = [r\"log $F$\", r\"$n$\", r\"log$\\mu$\", r\"log$\\sigma$\", r\"log$E_{\\mathrm{max}}$\", r\"log$E_{\\mathrm{min}}$\", r\"$\\gamma$\", r\"DM$_\\mathrm{halo}$\", r\"$H_0$\"]\n",
    "# filenames = ['../mcmc/MC_no_u7']\n",
    "\n",
    "labels = [\"lmean\", \"lsigma\", \"lEmax\", \"lEmin\", \"gamma\", \"DMhalo\", \"H0\"]\n",
    "filenames = ['../mcmc/MC_log_halo3']\n",
    "\n",
    "samples = []\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    reader = emcee.backends.HDFBackend(filename + '.h5')\n",
    "    samples.append(reader.get_chain())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negate $\\alpha$\n",
    "\n",
    "- In our code we assume $\\alpha$ is negative and so $\\alpha=2$ here corresponds to a negative spectral index.\n",
    "- So here, we change that to a negative for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make alpha negative\n",
    "a=-1\n",
    "for i, x in enumerate(labels):\n",
    "    if x == r\"$\\alpha$\":\n",
    "        a = i\n",
    "\n",
    "if a != -1:\n",
    "    for sample in samples:\n",
    "        sample[:,:,a] = -sample[:,:,a]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make F linear\n",
    "# a=-1\n",
    "# for i, x in enumerate(labels):\n",
    "#     if x == r\"log$_{10}(F)$\":\n",
    "#         a = i\n",
    "\n",
    "# if a != -1:\n",
    "#     for sample in samples:\n",
    "#         sample[:,:,a] = 10**sample[:,:,a]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot walkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,sample in enumerate(samples):\n",
    "    fig, axes = plt.subplots(sample.shape[2], 1, figsize=(20,30), sharex=True)\n",
    "    plt.title(\"Sample: \" + filenames[j])\n",
    "    for i,ax in enumerate(axes):\n",
    "        for k in range(sample.shape[1]):\n",
    "            ax.plot(sample[:,k,i], '.-', label=str(k))\n",
    "\n",
    "        ax.set_ylabel(labels[i])\n",
    "    \n",
    "    axes[-1].set_xlabel(\"Step number\")\n",
    "    axes[-1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burnin\n",
    "\n",
    "Here we present different methods to get the burnin from https://emcee.readthedocs.io/en/stable/tutorials/autocorr/#a-more-realistic-example however we note that in actuality it is generally easier and more useful to specify burnin=200 or something similar which is done further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_pow_two(n):\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        i = i << 1\n",
    "    return i\n",
    "\n",
    "def autocorr_func_1d(x, norm=True):\n",
    "    x = np.atleast_1d(x)\n",
    "    if len(x.shape) != 1:\n",
    "        raise ValueError(\"invalid dimensions for 1D autocorrelation function\")\n",
    "    n = next_pow_two(len(x))\n",
    "\n",
    "    # Compute the FFT and then (from that) the auto-correlation function\n",
    "    f = np.fft.fft(x - np.mean(x), n=2 * n)\n",
    "    acf = np.fft.ifft(f * np.conjugate(f))[: len(x)].real\n",
    "    acf /= 4 * n\n",
    "\n",
    "    # Optionally normalize\n",
    "    if norm and acf[0] != 0:\n",
    "        acf /= acf[0]\n",
    "\n",
    "    return acf\n",
    "\n",
    "# Automated windowing procedure following Sokal (1989)\n",
    "def auto_window(taus, c):\n",
    "    m = np.arange(len(taus)) < c * taus\n",
    "    if np.any(m):\n",
    "        return np.argmin(m)\n",
    "    return len(taus) - 1\n",
    "\n",
    "def autocorr(y, c=5.0):\n",
    "    f = np.zeros(y.shape[1])\n",
    "    for yy in y:\n",
    "        f += autocorr_func_1d(yy)\n",
    "    f /= len(y)\n",
    "    taus = 2.0 * np.cumsum(f) - 1.0\n",
    "    window = auto_window(taus, c)\n",
    "    return taus[window]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discard walkers\n",
    "\n",
    "- Discards any walkers that do not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reject walkers with bad autocorrelation values\n",
    "def auto_corr_rej(samples, burnin=0):\n",
    "    good_samples = []\n",
    "\n",
    "    # Loop through each sample and generate a list of good walkers and bad walkers\n",
    "    for j,sample in enumerate(samples): \n",
    "        # burnin=200\n",
    "        good_walkers = []\n",
    "        bad_walkers = []\n",
    "\n",
    "\n",
    "        # for i in range(sample.shape[1]):\n",
    "        #     # if np.all(sample[burnin:burnin+30,i,0] == sample[burnin,i,0]):\n",
    "        #     if ( np.std(sample[burnin:burnin+30,i,0] ) )\n",
    "        #         bad_walkers.append(i)\n",
    "        #     else:\n",
    "        #         good_walkers.append(i)\n",
    "\n",
    "        # Loop through each walker in the current sample\n",
    "        for i in range(sample.shape[1]):\n",
    "            bad = False\n",
    "\n",
    "            # Loop through each parameter for the walker\n",
    "            for k in range(sample.shape[2]):\n",
    "\n",
    "                # If any of the parameters have a bad autocorrelation function then set as a bad walker\n",
    "                acf = autocorr_func_1d(sample[burnin:,i,k], norm=False)\n",
    "                if np.max(acf) < 1e-10:\n",
    "                    bad = True\n",
    "                    break\n",
    "\n",
    "            if bad:\n",
    "                bad_walkers.append(i)\n",
    "            else:\n",
    "                good_walkers.append(i)\n",
    "            \n",
    "            # fig = plt.figure()\n",
    "            # plt.title(str(j) + \", \" + str(i))\n",
    "            # ax = fig.add_subplot(1,1,1)\n",
    "            # ax.plot(acf)\n",
    "\n",
    "        print(\"Discarded walkers for sample \" + str(j) + \": \" + str(bad_walkers))\n",
    "\n",
    "        # Add the new sample with the bad walkers discarded to the good_samples list\n",
    "        good_samples.append(sample[burnin:,good_walkers,:])\n",
    "\n",
    "    return good_samples\n",
    "\n",
    "# Reject walkers with small standard deviations\n",
    "def std_rej(samples, burnin=0):\n",
    "    good_samples = []\n",
    "\n",
    "    if not type(burnin) == list:\n",
    "        burnin = [burnin for i in range(len(samples))]\n",
    "\n",
    "    # Loop through each sample\n",
    "    for i, sample in enumerate(samples):\n",
    "        bad_walkers = []\n",
    "        good_walkers = []\n",
    "\n",
    "        # For each parameter\n",
    "        for k in range(sample.shape[2]):\n",
    "            sd = []\n",
    "\n",
    "            # Loop through every walker and get a list of the standard deviations\n",
    "            for j in range(sample.shape[1]):\n",
    "                sd.append(np.std(sample[burnin[i]:burnin[i]+100,j,k]))\n",
    "            \n",
    "            # Normalise standard deviation\n",
    "            sd = sd / np.max(sd)\n",
    "\n",
    "            # Flag any walkers with standard deviations less than 1e-2\n",
    "            # bad_walkers = np.flatnonzero(sd < 1e-2)\n",
    "            # temp = []\n",
    "            # for m in range(len(sd)):\n",
    "            #     if sd[m] < 1e-2:\n",
    "            #         bad_walkers.append(m)\n",
    "\n",
    "        # bad_walkers.append(31)\n",
    "        # bad_walkers.append(5)\n",
    "        bad_walkers.append(18)\n",
    "        bad_walkers = np.unique(np.array(bad_walkers))\n",
    "\n",
    "        print(\"Discarded walkers for sample \" + str(i) + \": \" + str(bad_walkers))\n",
    "        for l in range(sample.shape[1]):\n",
    "            if l not in bad_walkers:\n",
    "                good_walkers.append(l)\n",
    "\n",
    "        # Add the new sample with the bad walkers discarded to the good_samples list\n",
    "        good_samples.append(sample[:,good_walkers,:])\n",
    "    \n",
    "    return good_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples = std_rej(samples, burnin=0)\n",
    "# good_samples = samples\n",
    "# _ = auto_corr_rej(samples, burnin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,sample in enumerate(good_samples):\n",
    "    fig, axes = plt.subplots(sample.shape[2], 1, figsize=(20,30), sharex=True)\n",
    "    plt.title(\"Sample: \" + filenames[j])\n",
    "    for i,ax in enumerate(axes):\n",
    "        for k in range(sample.shape[1]):\n",
    "            ax.plot(sample[:,k,i], '.-', label=str(k))\n",
    "\n",
    "        ax.set_ylabel(labels[i])\n",
    "    \n",
    "    axes[-1].set_xlabel(\"Step number\")\n",
    "    axes[-1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the autocorrelation time to estimate the burnin once the bad walkers have been discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = []\n",
    "for sample in good_samples:\n",
    "    # Compute the estimators for a few different chain lengths\n",
    "    N = np.exp(np.linspace(np.log(10), np.log(sample.shape[0]), 10)).astype(int)\n",
    "    new = np.empty(len(N))\n",
    "    for i, n in enumerate(N):\n",
    "        new[i] = autocorr(sample[:, :n, 0].T)\n",
    "\n",
    "    # Plot the comparisons\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    ax.loglog(N, new, \"o-\", label=\"new\")\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot(N, N / 50.0, \"--k\", label=r\"$\\tau = N/50$\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(\"number of samples, $N$\")\n",
    "    ax.set_ylabel(r\"$\\tau$ estimates\")\n",
    "    # ax.legend(fontsize=14);\n",
    "\n",
    "    burnin.append(int(1.5*new[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j,sample in enumerate(good_samples):\n",
    "#     fig, axes = plt.subplots(sample.shape[2], 1, figsize=(20,30), sharex=True)\n",
    "#     plt.title(\"Sample: \" + filenames[j])\n",
    "#     for i,ax in enumerate(axes):\n",
    "#         for k in range(sample.shape[1]):\n",
    "#             ax.plot(sample[burnin[j]:,k,i], '.-', label=str(k))\n",
    "\n",
    "#         ax.set_ylabel(labels[i])\n",
    "    \n",
    "#     axes[-1].set_xlabel(\"Step number\")\n",
    "#     axes[-1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement burnin and change priors\n",
    "\n",
    "- Changes prior to discard samples outside the specified prior range\n",
    "- Implements the burnin using either the predefined burnin or a constant specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce more restrictive priors on a parameter\n",
    "def change_priors(sample, param_num, max=np.inf, min=-np.inf):\n",
    "\n",
    "    condition = np.logical_and(sample[:,param_num] > min, sample[:,param_num] < max)\n",
    "    good_idxs = np.flatnonzero(condition)\n",
    "\n",
    "    return sample[good_idxs, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final sample without burnin and without bad walkers\n",
    "final_sample = [[] for i in range(samples[0].shape[2])]\n",
    "\n",
    "# print(burnin)\n",
    "burnin = (np.ones(len(good_samples)) * 200).astype(int)\n",
    "print(burnin)\n",
    "\n",
    "for j,sample in enumerate(good_samples):\n",
    "    for i in range(sample.shape[2]):\n",
    "        final_sample[i].append(sample[burnin[j]:,:,i].flatten())\n",
    "final_sample = np.array([np.hstack(final_sample[i]) for i in range(len(final_sample))]).T\n",
    "\n",
    "# final_sample = change_priors(final_sample, 5, min=38)\n",
    "# final_sample = change_priors(final_sample, 7, max=110.0)\n",
    "# final_sample = change_priors(final_sample, 9, max=80.0)\n",
    "# final_sample = change_priors(final_sample, 1, max=1.0, min=-3.5)\n",
    "\n",
    "print(final_sample.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cornerplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lmean = 2.1198711983468064\n",
    "DMhalo = np.log10(50.0)\n",
    "param_dict={'sfr_n': 0.8808527057055584, 'alpha': 0.7895161131856694, 'lmean': lmean, 'lsigma': 0.44944780033763343, \n",
    "            'lEmax': 41.18671139482926, 'lEmin': 39.81049090314043, 'gamma': -1.1558450520609953, 'H0': 54.6887137195215, 'DMhalo': DMhalo}\n",
    "\n",
    "truths = [param_dict[param] for param in labels]\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "titles = ['' for i in range(final_sample.shape[1])]\n",
    "corner.corner(final_sample,labels=labels, show_titles=True, titles=titles, \n",
    "              fig=fig,title_kwargs={\"fontsize\": 15},label_kwargs={\"fontsize\": 15}, \n",
    "              quantiles=[0.16,0.5,0.84], truths=truths);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point estimates\n",
    "\n",
    "- Use finer histogram binning than the corner plot\n",
    "- Obtain point estimates and confidence intervals using median / mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBins = 20\n",
    "win_len = int(nBins/10)\n",
    "CL = 0.68\n",
    "\n",
    "best_fit = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    hist, bin_edges, _ = ax.hist(final_sample[:,i], bins=nBins, density=True)\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    bins = -np.diff(bin_edges)/2.0 + bin_edges[1:]\n",
    "\n",
    "    ax.set_xlabel(labels[i])\n",
    "    ax.set_ylabel(\"P(\"+labels[i]+\")\")\n",
    "\n",
    "    # Use mode ordered\n",
    "    # ordered_idxs = np.argsort(hist)\n",
    "\n",
    "    # sum = hist[ordered_idxs[0]] * bin_width\n",
    "    # j = 1\n",
    "    # while(sum < 1-CL):\n",
    "    #     sum += hist[ordered_idxs[j]] * bin_width\n",
    "    #     j = j+1\n",
    "\n",
    "    # best = bins[ordered_idxs[-1]]\n",
    "    # lower = bins[np.min(ordered_idxs[j:])]\n",
    "    # upper = bins[np.max(ordered_idxs[j:])]\n",
    "\n",
    "    # Use median\n",
    "    best = np.quantile(final_sample[:,i], 0.5)\n",
    "    # best = bins[np.argmax(hist)]\n",
    "    lower = np.quantile(final_sample[:,i], 0.16)\n",
    "    upper = np.quantile(final_sample[:,i], 0.84)\n",
    "\n",
    "    best_fit[labels[i]] = best\n",
    "    u_lower = best - lower\n",
    "    u_upper = upper - best\n",
    "    ax.axvline(lower, color='r')\n",
    "    ax.axvline(best, color='r')\n",
    "    ax.axvline(upper, color='r')\n",
    "    # print(labels[i] + \": \" + str(best) + \" (-\" + str(u_lower) + \"/+\" + str(u_upper) + \")\")\n",
    "    print(rf'{labels[i]}: {best} (-{u_lower}/+{u_upper})')\n",
    "\n",
    "print(best_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsamps = np.linspace(3, np.log10(final_sample.shape[0]/10), 30)\n",
    "# nsamps = [int(10**x) for x in nsamps]\n",
    "# print(\"Number of samps: \" + str(nsamps))\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     # nsamps = []\n",
    "#     std = []\n",
    "#     for j in range(len(nsamps)):\n",
    "#         best = []\n",
    "#         nruns = int(final_sample.shape[0] / nsamps[j])\n",
    "#         for k in range(nruns):\n",
    "#             # best.append(np.quantile(final_sample[nsamps[j]*k:nsamps[j]*(k+1),i], 0.5))\n",
    "#             step = int(final_sample.shape[0]/nsamps[j])\n",
    "#             best.append(np.quantile(final_sample[k::step,i], 0.5))\n",
    "#         std.append(np.std(best))\n",
    "\n",
    "#     # print(labels[i] + \": \" + str(std))\n",
    "\n",
    "#     line = st.linregress(np.log10(nsamps),np.log10(std))\n",
    "#     x = np.linspace(nsamps[0], nsamps[-1], 50)\n",
    "#     y = 1/np.sqrt(x)\n",
    "#     y = y / y[0] * std[0]\n",
    "#     y = 10**(line.slope*np.log10(x) + line.intercept)\n",
    "#     # print(line.slope)\n",
    "#     print(labels[i] + \": \" + str(10**(line.slope*np.log10(final_sample.shape[0]) + line.intercept)))\n",
    "#     print(str(line.slope))\n",
    "#     fig = plt.figure(figsize=(6,4))\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "#     ax.plot(nsamps, std)\n",
    "#     ax.loglog(x,y)\n",
    "#     ax.set_xlabel(\"Number of samples\")\n",
    "#     ax.set_ylabel(\"Standard deviation\")\n",
    "#     ax.set_title(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load surveys and grids\n",
    "\n",
    "- Loads the surveys and grids with the best fit parameters from above.\n",
    "- Plots P(DM) and DMEG weights for each FRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_names = [\n",
    "    # \"FAST2\",\n",
    "    # \"FAST2_old\"\n",
    "    \"DSA\",\n",
    "    # \"FAST\", \n",
    "    # \"CRAFT_class_I_and_II\", \n",
    "    # \"private_CRAFT_ICS_892_14\", \n",
    "    # \"private_CRAFT_ICS_1300_14\", \n",
    "    # \"private_CRAFT_ICS_1632_14\", \n",
    "    # \"parkes_mb_class_I_and_II\"\n",
    "]\n",
    "# rs_names = [\"CHIME/CHIME_decbin_0_of_6\",\n",
    "#             \"CHIME/CHIME_decbin_1_of_6\",\n",
    "#             \"CHIME/CHIME_decbin_2_of_6\",\n",
    "#             \"CHIME/CHIME_decbin_3_of_6\",\n",
    "#             \"CHIME/CHIME_decbin_4_of_6\",\n",
    "#             \"CHIME/CHIME_decbin_5_of_6\"]\n",
    "rs_names = []\n",
    "\n",
    "state = parameters.State()\n",
    "state.set_astropy_cosmo(Planck18) \n",
    "# state.update_params(best_fit)\n",
    "# state.update_param('luminosity_function', 2)\n",
    "# state.update_param('alpha_method', 0)\n",
    "# state.update_param('sfr_n', 1.36)\n",
    "# state.update_param('alpha', 1.5)\n",
    "# state.update_param('lmean', 1.97)\n",
    "# state.update_param('lsigma', 0.92)\n",
    "# state.update_param('lEmax', 41.3)\n",
    "# state.update_param('gamma', -0.63)\n",
    "# state.update_param('H0', 70.0)\n",
    "# state.update_param('DMhalo', 50.0)\n",
    "\n",
    "if len(s_names) != 0:\n",
    "    surveys, grids = loading.surveys_and_grids(survey_names = s_names, init_state=state, repeaters=False, nz=500, ndm=1400)\n",
    "else:\n",
    "    surveys = []\n",
    "    grids = []\n",
    "\n",
    "if len(rs_names) != 0:\n",
    "    rep_surveys, rep_grids = loading.surveys_and_grids(survey_names = rs_names, init_state=state, repeaters=True, nz=500, ndm=1400)\n",
    "    for s,g in zip(rep_surveys, rep_grids):\n",
    "        surveys.append(s)\n",
    "        grids.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newC, llc = it.minimise_const_only(None, grids, surveys)\n",
    "llsum = 0\n",
    "for s,g in zip(surveys, grids):\n",
    "\n",
    "    g.state.FRBdemo.lC = newC\n",
    "    \n",
    "    # Calc pdm\n",
    "    rates=g.rates\n",
    "    dmvals=g.dmvals\n",
    "    pdm=np.sum(rates,axis=0)\n",
    "\n",
    "    # # Calc psnr\n",
    "    # min = s.SNRTHRESHs[0]\n",
    "    # max = np.max(s.SNRs)\n",
    "    # snrs = np.linspace(min,max, 50)\n",
    "    # psnr = get_psnr(snrs, s, g)\n",
    "    \n",
    "    # Plot pdm + snr\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(s.name)\n",
    "    ax.set_xlabel(\"DM\")\n",
    "    ax.set_ylabel(\"P(DM)\")\n",
    "    ax.set_xlim(xmax=3000)\n",
    "    ax.plot(dmvals, pdm)\n",
    "    ax.vlines(s.DMEGs, np.zeros(len(s.DMs)), np.max(pdm)*np.ones(len(s.DMs)), ls='--', colors='r')\n",
    "    \n",
    "    # ax = fig.add_subplot(1,2,2)\n",
    "    # ax.set_xlabel(\"log SNR\")\n",
    "    # ax.set_ylabel(\"log P(SNR)\")\n",
    "    # ax.plot(np.log10(snrs), np.log10(psnr))\n",
    "    # ax.vlines(np.log10(s.SNRs), np.min(np.log10(psnr))*np.ones(len(s.SNRs)), np.max(np.log10(psnr))*np.ones(len(s.SNRs)), ls='--', colors='r')\n",
    "\n",
    "    # Get expected and observed\n",
    "    expected=it.CalculateIntegral(g.rates,s)\n",
    "    expected *= 10**g.state.FRBdemo.lC\n",
    "    observed=s.NORM_FRB\n",
    "\n",
    "    print(s.name + \" - expected, observed: \" + str(expected) + \", \" + str(observed))\n",
    "\n",
    "    llsum += it.get_log_likelihood(g,s,Pn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uDMGs = 0.5\n",
    "# DMhalo = 100.0\n",
    "\n",
    "fig = plt.figure(figsize=(10,6*len(s_names)))\n",
    "\n",
    "for j,(s,g) in enumerate(zip(surveys, grids)):\n",
    "    ax = fig.add_subplot(len(surveys),1,j+1)\n",
    "    plt.title(s.name)\n",
    "    ax.set_xlabel('DM')\n",
    "    ax.set_ylabel('Weight')\n",
    "\n",
    "    # s.DMhalo = DMhalo\n",
    "    # s.init_DMEG(DMhalo)\n",
    "\n",
    "    dmvals=g.dmvals\n",
    "    DMobs=s.DMEGs\n",
    "\n",
    "        # calc_DMG_weights(DMEGs, DMhalos, DM_ISMs, dmvals, sigma_ISM=0.5, sigma_halo=15.0, percent_ISM=True)\n",
    "    dm_weights, iweights = it.calc_DMG_weights(DMobs, s.DMhalos, s.DMGs, dmvals, uDMGs)\n",
    "\n",
    "    pdm = np.sum(g.rates, axis=0)\n",
    "    pdm = pdm / np.max(pdm) * np.max(dm_weights[0])\n",
    "\n",
    "    for i in range(len(DMobs)):\n",
    "        ax.plot(dmvals[iweights[i]], dm_weights[i], '.-', label=s.frbs[\"TNS\"][i] + \" \" + str(s.DMGs[i]))\n",
    "\n",
    "    ax.plot(dmvals, pdm) # Upper limit is not correct because grid has not been updated so efficiencies have not been recalc'd\n",
    "    ax.set_xlim(right=3000)\n",
    "    ax.legend()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
