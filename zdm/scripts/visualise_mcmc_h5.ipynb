{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import emcee\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfile = \"../tests/files/real_mini_mcmc.json\"\n",
    "with open(pfile) as f:\n",
    "    mcmc_dict = json.load(f)\n",
    "\n",
    "# Select from dictionary the necessary parameters to be changed\n",
    "# labels = mcmc_dict['mcmc']['parameter_order']\n",
    "\n",
    "# labels = [r\"$n_{\\mathrm{sfr}}$\", r\"$\\alpha$\", r\"log$_{10}(\\mu_{\\rm{host}})$\", r\"log$_{10}(\\sigma_{\\rm{host}})$\", r\"log$_{10}(E_{\\mathrm{max}})$\", r\"$\\gamma$\", r\"$H_0$\"]\n",
    "# filenames = [\"../mcmc/Hoffmann2023_CRAFT_no_191228\", \"../mcmc/Hoffmann2023_CRAFT_no_191228_2\"]\n",
    "# filenames = [\"../mcmc/Hoffmann2023_CRAFT2\", \"../mcmc/Hoffmann2023_CRAFT3\"]\n",
    "# filenames = [\"../mcmc/Hoffmann2023_exact_no_191228\", \"../mcmc/Hoffmann2023_exact_no_191228_2\", \n",
    "#              \"../mcmc/Hoffmann2023_exact_no_191228_3\", \"../mcmc/Hoffmann2023_exact_no_191228_4\", \n",
    "#              \"../mcmc/Hoffmann2023_exact_no_191228_5\", \"../mcmc/Hoffmann2023_exact_no_191228_6\",\n",
    "#              \"../mcmc/Hoffmann2023_exact_no_191228_7\", \"../mcmc/Hoffmann2023_exact_no_191228_8\"]\n",
    "# filenames = [\"../mcmc/Hoffmann2023_exact\", \"../mcmc/Hoffmann2023_exact2\", \"../mcmc/Hoffmann2023_exact3\", \"../mcmc/Hoffmann2023_exact5\", \n",
    "#             \"../mcmc/Hoffmann2023_exact6\", \"../mcmc/Hoffmann2023_exact7\", \"../mcmc/Hoffmann2023_exact8\"]\n",
    "\n",
    "# labels = [r\"$n_{\\mathrm{sfr}}$\", r\"$\\alpha$\", r\"log$_{10}(\\mu_{\\rm{host}})$\", r\"log$_{10}(\\sigma_{\\rm{host}})$\", r\"log$_{10}(E_{\\mathrm{max}})$\", r\"$\\gamma$\", r\"$H_0$\", r\"DM$_{\\rm{halo}}$\"]\n",
    "# labels = [\"sfr_n\", \"alpha\", \"lmean\", \"lsigma\", \"lEmax\", \"gamma\", \"H0\", \"DMhalo\"]\n",
    "# filenames = ['../mcmc/DSA5', '../mcmc/DSA7']\n",
    "# filenames = ['../mcmc/DSA2', '../mcmc/DSA3 ']\n",
    "\n",
    "# labels = [r\"log$_{10}(F)$\", r\"$n_{\\mathrm{sfr}}$\", r\"$\\alpha$\", r\"log$_{10}(\\mu_{\\rm{host}})$\", r\"log$_{10}(\\sigma_{\\rm{host}})$\", \n",
    "#           r\"log$_{10}(E_{\\mathrm{max}})$\", r\"$\\gamma$\", r\"$H_0$\", r\"log$_{10}(E_{\\rm{min}})$\", r\"DM$_{\\rm{halo}}$\"]\n",
    "# labels = [\"logF\", \"sfr_n\", \"alpha\", \"lmean\", \"lsigma\", \"lEmax\", \"gamma\", \"H0\", \"lEmin\", \"DMhalo\"]\n",
    "# filenames = ['../mcmc/DSA_FAST_CRAFT', '../mcmc/DSA_FAST_CRAFT_2']\n",
    "\n",
    "labels = [r\"$n_{\\mathrm{sfr}}$\", r\"$\\alpha$\", r\"log$_{10}(\\mu_{\\rm{host}})$\", r\"log$_{10}(\\sigma_{\\rm{host}})$\", \n",
    "          r\"log$_{10}(E_{\\mathrm{max}})$\", r\"$\\gamma$\", r\"$H_0$\", r\"log$_{10}(E_{\\rm{min}})$\", r\"DM$_{\\rm{halo}}$\"]\n",
    "labels = [\"sfr_n\", \"alpha\", \"lmean\", \"lsigma\", \"lEmax\", \"gamma\", \"H0\", \"lEmin\", \"DMhalo\"]\n",
    "filenames = ['../mcmc/DSA_FAST_CRAFT_3']\n",
    "\n",
    "# labels = [r\"log$_{10}(F)$\", r\"$n_{\\mathrm{sfr}}$\", r\"$\\alpha$\", r\"log$_{10}(\\mu_{\\rm{host}})$\", r\"log$_{10}(\\sigma_{\\rm{host}})$\", r\"log$_{10}(E_{\\mathrm{max}})$\", r\"$\\gamma$\", r\"$H_0$\", r\"DM$_{\\rm{halo}}$\"]\n",
    "# filenames = ['../mcmc/DSA6']\n",
    "\n",
    "# labels = [\"sfr_n\", \"alpha\", \"lmean\", \"lsigma\", \"lEmax\", \"gamma\", \"H0\", \"lEmin\"]\n",
    "# filenames = ['../mcmc/FASTnpCRAFT6']\n",
    "# filenames = ['../mcmc/FASTnpCRAFT2', '../mcmc/FASTnpCRAFT3', '../mcmc/FASTnpCRAFT4']\n",
    "\n",
    "# labels = [\"sfr_n\", \"alpha\", \"lmean\", \"lsigma\", \"lEmax\", \"gamma\", \"H0\", \"lEmin\", \"DMhalo\"]\n",
    "# filenames = ['../mcmc/DSA4']\n",
    "\n",
    "samples = []\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    reader = emcee.backends.HDFBackend(filename + '.h5')\n",
    "    samples.append(reader.get_chain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make alpha negative\n",
    "a=-1\n",
    "for i, x in enumerate(labels):\n",
    "    if x == r\"$\\alpha$\":\n",
    "        a = i\n",
    "\n",
    "if a != -1:\n",
    "    for sample in samples:\n",
    "        sample[:,:,a] = -sample[:,:,a]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make F linear\n",
    "# a=-1\n",
    "# for i, x in enumerate(labels):\n",
    "#     if x == r\"log$_{10}(F)$\":\n",
    "#         a = i\n",
    "\n",
    "# if a != -1:\n",
    "#     for sample in samples:\n",
    "#         sample[:,:,a] = 10**sample[:,:,a]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,sample in enumerate(samples):\n",
    "    fig, axes = plt.subplots(sample.shape[2], 1, figsize=(20,30), sharex=True)\n",
    "    plt.title(\"Sample: \" + filenames[j])\n",
    "    for i,ax in enumerate(axes):\n",
    "        for k in range(sample.shape[1]):\n",
    "            ax.plot(sample[:,k,i], '.-', label=str(k))\n",
    "\n",
    "        ax.set_ylabel(labels[i])\n",
    "    \n",
    "    axes[-1].set_xlabel(\"Step number\")\n",
    "    axes[-1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://emcee.readthedocs.io/en/stable/tutorials/autocorr/#a-more-realistic-example\n",
    "def next_pow_two(n):\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        i = i << 1\n",
    "    return i\n",
    "\n",
    "def autocorr_func_1d(x, norm=True):\n",
    "    x = np.atleast_1d(x)\n",
    "    if len(x.shape) != 1:\n",
    "        raise ValueError(\"invalid dimensions for 1D autocorrelation function\")\n",
    "    n = next_pow_two(len(x))\n",
    "\n",
    "    # Compute the FFT and then (from that) the auto-correlation function\n",
    "    f = np.fft.fft(x - np.mean(x), n=2 * n)\n",
    "    acf = np.fft.ifft(f * np.conjugate(f))[: len(x)].real\n",
    "    acf /= 4 * n\n",
    "\n",
    "    # Optionally normalize\n",
    "    if norm and acf[0] != 0:\n",
    "        acf /= acf[0]\n",
    "\n",
    "    return acf\n",
    "\n",
    "# Automated windowing procedure following Sokal (1989)\n",
    "def auto_window(taus, c):\n",
    "    m = np.arange(len(taus)) < c * taus\n",
    "    if np.any(m):\n",
    "        return np.argmin(m)\n",
    "    return len(taus) - 1\n",
    "\n",
    "def autocorr(y, c=5.0):\n",
    "    f = np.zeros(y.shape[1])\n",
    "    for yy in y:\n",
    "        f += autocorr_func_1d(yy)\n",
    "    f /= len(y)\n",
    "    taus = 2.0 * np.cumsum(f) - 1.0\n",
    "    window = auto_window(taus, c)\n",
    "    return taus[window]\n",
    "\n",
    "# def autocorr_ml(y, thin=1, c=5.0):\n",
    "#     # Compute the initial estimate of tau using the standard method\n",
    "#     init = autocorr(y, c=c)\n",
    "#     z = y[:, ::thin]\n",
    "#     N = z.shape[1]\n",
    "\n",
    "#     # Build the GP model\n",
    "#     tau = max(1.0, init / thin)\n",
    "#     kernel = terms.RealTerm(\n",
    "#         np.log(0.9 * np.var(z)),\n",
    "#         -np.log(tau),\n",
    "#         bounds=[(-5.0, 5.0), (-np.log(N), 0.0)],\n",
    "#     )\n",
    "#     kernel += terms.RealTerm(\n",
    "#         np.log(0.1 * np.var(z)),\n",
    "#         -np.log(0.5 * tau),\n",
    "#         bounds=[(-5.0, 5.0), (-np.log(N), 0.0)],\n",
    "#     )\n",
    "#     gp = celerite.GP(kernel, mean=np.mean(z))\n",
    "#     gp.compute(np.arange(z.shape[1]))\n",
    "\n",
    "    # # Define the objective\n",
    "    # def nll(p):\n",
    "    #     # Update the GP model\n",
    "    #     gp.set_parameter_vector(p)\n",
    "\n",
    "    #     # Loop over the chains and compute likelihoods\n",
    "    #     v, g = zip(*(gp.grad_log_likelihood(z0, quiet=True) for z0 in z))\n",
    "\n",
    "    #     # Combine the datasets\n",
    "    #     return -np.sum(v), -np.sum(g, axis=0)\n",
    "\n",
    "    # # Optimize the model\n",
    "    # p0 = gp.get_parameter_vector()\n",
    "    # bounds = gp.get_parameter_bounds()\n",
    "    # soln = minimize(nll, p0, jac=True, bounds=bounds)\n",
    "    # gp.set_parameter_vector(soln.x)\n",
    "\n",
    "    # # Compute the maximum likelihood tau\n",
    "    # a, c = kernel.coefficients[:2]\n",
    "    # tau = thin * 2 * np.sum(a / c) / np.sum(a)\n",
    "    # return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reject walkers with bad autocorrelation values\n",
    "def auto_corr_rej(samples, burnin=0):\n",
    "    good_samples = []\n",
    "\n",
    "    # Loop through each sample and generate a list of good walkers and bad walkers\n",
    "    for j,sample in enumerate(samples): \n",
    "        # burnin=200\n",
    "        good_walkers = []\n",
    "        bad_walkers = []\n",
    "\n",
    "\n",
    "        # for i in range(sample.shape[1]):\n",
    "        #     # if np.all(sample[burnin:burnin+30,i,0] == sample[burnin,i,0]):\n",
    "        #     if ( np.std(sample[burnin:burnin+30,i,0] ) )\n",
    "        #         bad_walkers.append(i)\n",
    "        #     else:\n",
    "        #         good_walkers.append(i)\n",
    "\n",
    "        # Loop through each walker in the current sample\n",
    "        for i in range(sample.shape[1]):\n",
    "            bad = False\n",
    "\n",
    "            # Loop through each parameter for the walker\n",
    "            for k in range(sample.shape[2]):\n",
    "\n",
    "                # If any of the parameters have a bad autocorrelation function then set as a bad walker\n",
    "                acf = autocorr_func_1d(sample[burnin:,i,k], norm=False)\n",
    "                if np.max(acf) < 1e-10:\n",
    "                    bad = True\n",
    "                    break\n",
    "\n",
    "            if bad:\n",
    "                bad_walkers.append(i)\n",
    "            else:\n",
    "                good_walkers.append(i)\n",
    "            \n",
    "            # fig = plt.figure()\n",
    "            # plt.title(str(j) + \", \" + str(i))\n",
    "            # ax = fig.add_subplot(1,1,1)\n",
    "            # ax.plot(acf)\n",
    "\n",
    "        print(\"Discarded walkers for sample \" + str(j) + \": \" + str(bad_walkers))\n",
    "\n",
    "        # Add the new sample with the bad walkers discarded to the good_samples list\n",
    "        good_samples.append(sample[burnin:,good_walkers,:])\n",
    "\n",
    "    return good_samples\n",
    "\n",
    "# Reject walkers with small standard deviations\n",
    "def std_rej(samples, burnin=0):\n",
    "    good_samples = []\n",
    "\n",
    "    if not type(burnin) == list:\n",
    "        burnin = [burnin for i in range(len(samples))]\n",
    "\n",
    "    # Loop through each sample\n",
    "    for i, sample in enumerate(samples):\n",
    "        bad_walkers = []\n",
    "        good_walkers = []\n",
    "\n",
    "        # For each parameter\n",
    "        for k in range(sample.shape[2]):\n",
    "            sd = []\n",
    "\n",
    "            # Loop through every walker and get a list of the standard deviations\n",
    "            for j in range(sample.shape[1]):\n",
    "                sd.append(np.std(sample[burnin[i]:burnin[i]+100,j,k]))\n",
    "            \n",
    "            # Normalise standard deviation\n",
    "            sd = sd / np.max(sd)\n",
    "\n",
    "            # Flag any walkers with standard deviations less than 1e-2\n",
    "            # bad_walkers = np.flatnonzero(sd < 1e-2)\n",
    "            # temp = []\n",
    "            for m in range(len(sd)):\n",
    "                if sd[m] < 1e-2:\n",
    "                    bad_walkers.append(m)\n",
    "        \n",
    "        bad_walkers = np.unique(np.array(bad_walkers))\n",
    "\n",
    "        print(\"Discarded walkers for sample \" + str(i) + \": \" + str(bad_walkers))\n",
    "        for l in range(sample.shape[1]):\n",
    "            if l not in bad_walkers:\n",
    "                good_walkers.append(l)\n",
    "\n",
    "        # Add the new sample with the bad walkers discarded to the good_samples list\n",
    "        good_samples.append(sample[burnin[i]:,good_walkers,:])\n",
    "    \n",
    "    return good_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples = std_rej(samples, burnin=0)\n",
    "# good_samples = samples\n",
    "# _ = auto_corr_rej(samples, burnin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = []\n",
    "for sample in good_samples:\n",
    "    # Compute the estimators for a few different chain lengths\n",
    "    N = np.exp(np.linspace(np.log(10), np.log(sample.shape[0]), 10)).astype(int)\n",
    "    new = np.empty(len(N))\n",
    "    for i, n in enumerate(N):\n",
    "        new[i] = autocorr(sample[:, :n, 0].T)\n",
    "\n",
    "    # Plot the comparisons\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    ax.loglog(N, new, \"o-\", label=\"new\")\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot(N, N / 50.0, \"--k\", label=r\"$\\tau = N/50$\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(\"number of samples, $N$\")\n",
    "    ax.set_ylabel(r\"$\\tau$ estimates\")\n",
    "    # ax.legend(fontsize=14);\n",
    "\n",
    "    burnin.append(int(1.5*new[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_samples = std_rej(samples, burnin=200)\n",
    "# _ = auto_corr_rej(samples, burnin=burnin)\n",
    "# burnin = (np.ones(len(good_samples)) * 100).astype(int)\n",
    "# print(burnin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,sample in enumerate(good_samples):\n",
    "    fig, axes = plt.subplots(sample.shape[2], 1, figsize=(20,30), sharex=True)\n",
    "    plt.title(\"Sample: \" + filenames[j])\n",
    "    for i,ax in enumerate(axes):\n",
    "        for k in range(sample.shape[1]):\n",
    "            ax.plot(sample[burnin[j]:,k,i], '.-', label=str(k))\n",
    "\n",
    "        ax.set_ylabel(labels[i])\n",
    "    \n",
    "    axes[-1].set_xlabel(\"Step number\")\n",
    "    axes[-1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce more restrictive priors on a parameter\n",
    "def change_priors(sample, param_num, max=np.inf, min=-np.inf):\n",
    "\n",
    "    condition = np.logical_and(sample[:,param_num] > min, sample[:,param_num] < max)\n",
    "    good_idxs = np.flatnonzero(condition)\n",
    "\n",
    "    return sample[good_idxs, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final sample without burnin and without bad walkers\n",
    "final_sample = [[] for i in range(samples[0].shape[2])]\n",
    "\n",
    "print(burnin)\n",
    "# burnin = (np.ones(len(good_samples)) * 60).astype(int)\n",
    "# print(burnin)\n",
    "\n",
    "for j,sample in enumerate(good_samples):\n",
    "    for i in range(sample.shape[2]):\n",
    "        final_sample[i].append(sample[burnin[j]:,:,i].flatten())\n",
    "final_sample = np.array([np.hstack(final_sample[i]) for i in range(len(final_sample))]).T\n",
    "\n",
    "# final_sample = change_priors(final_sample, 8, min=20.0)\n",
    "# final_sample = change_priors(final_sample, 7, max=110.0)\n",
    "# final_sample = change_priors(final_sample, 9, max=80.0)\n",
    "# final_sample = change_priors(final_sample, 1, max=1.0, min=-3.5)\n",
    "\n",
    "print(final_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "titles = ['' for i in range(final_sample.shape[1])]\n",
    "corner.corner(final_sample,labels=labels, show_titles=True, titles=titles, fig=fig,title_kwargs={\"fontsize\": 15},label_kwargs={\"fontsize\": 15}, quantiles=[0.16,0.5,0.84]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBins = 30\n",
    "win_len = int(nBins/10)\n",
    "CL = 0.68\n",
    "\n",
    "best_fit = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    hist, bin_edges, _ = ax.hist(final_sample[:,i], bins=nBins, density=True)\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    bins = -np.diff(bin_edges)/2.0 + bin_edges[1:]\n",
    "\n",
    "    ax.set_xlabel(labels[i])\n",
    "    ax.set_ylabel(\"P(\"+labels[i]+\")\")\n",
    "\n",
    "    # Use mode ordered\n",
    "    # ordered_idxs = np.argsort(hist)\n",
    "\n",
    "    # sum = hist[ordered_idxs[0]] * bin_width\n",
    "    # j = 1\n",
    "    # while(sum < 1-CL):\n",
    "    #     sum += hist[ordered_idxs[j]] * bin_width\n",
    "    #     j = j+1\n",
    "\n",
    "    # best = bins[ordered_idxs[-1]]\n",
    "    # lower = bins[np.min(ordered_idxs[j:])]\n",
    "    # upper = bins[np.max(ordered_idxs[j:])]\n",
    "\n",
    "    # Use median\n",
    "    best = np.quantile(final_sample[:,i], 0.5)\n",
    "    # best = bins[np.argmax(hist)]\n",
    "    lower = np.quantile(final_sample[:,i], 0.16)\n",
    "    upper = np.quantile(final_sample[:,i], 0.84)\n",
    "\n",
    "    best_fit[labels[i]] = best\n",
    "    u_lower = best - lower\n",
    "    u_upper = upper - best\n",
    "    ax.axvline(lower, color='r')\n",
    "    ax.axvline(best, color='r')\n",
    "    ax.axvline(upper, color='r')\n",
    "    # print(labels[i] + \": \" + str(best) + \" (-\" + str(u_lower) + \"/+\" + str(u_upper) + \")\")\n",
    "    print(rf'{labels[i]}: {best} (-{u_lower}/+{u_upper})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = np.linspace(3, np.log10(final_sample.shape[0]/10), 30)\n",
    "nsamps = [int(10**x) for x in nsamps]\n",
    "print(\"Number of samps: \" + str(nsamps))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    # nsamps = []\n",
    "    std = []\n",
    "    for j in range(len(nsamps)):\n",
    "        best = []\n",
    "        nruns = int(final_sample.shape[0] / nsamps[j])\n",
    "        for k in range(nruns):\n",
    "            # best.append(np.quantile(final_sample[nsamps[j]*k:nsamps[j]*(k+1),i], 0.5))\n",
    "            step = int(final_sample.shape[0]/nsamps[j])\n",
    "            best.append(np.quantile(final_sample[k::step,i], 0.5))\n",
    "        std.append(np.std(best))\n",
    "\n",
    "    # print(labels[i] + \": \" + str(std))\n",
    "\n",
    "    line = st.linregress(np.log10(nsamps),np.log10(std))\n",
    "    x = np.linspace(nsamps[0], nsamps[-1], 50)\n",
    "    y = 1/np.sqrt(x)\n",
    "    y = y / y[0] * std[0]\n",
    "    y = 10**(line.slope*np.log10(x) + line.intercept)\n",
    "    # print(line.slope)\n",
    "    print(labels[i] + \": \" + str(10**(line.slope*np.log10(final_sample.shape[0]) + line.intercept)))\n",
    "    print(str(line.slope))\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    ax.plot(nsamps, std)\n",
    "    ax.loglog(x,y)\n",
    "    ax.set_xlabel(\"Number of samples\")\n",
    "    ax.set_ylabel(\"Standard deviation\")\n",
    "    ax.set_title(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zdm import survey\n",
    "from zdm import cosmology as cos\n",
    "from zdm.craco import loading\n",
    "from zdm.misc_functions import *\n",
    "import zdm.iteration as it\n",
    "from zdm.MCMC import calc_log_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='DSA_FAST_CRAFT'\n",
    "\n",
    "cos.init_dist_measures()\n",
    "state = loading.set_state()\n",
    "\n",
    "# get the grid of p(DM|z)\n",
    "zDMgrid, zvals,dmvals=get_zdm_grid(state,new=True,plot=False,method='analytic',save=True,datdir='MCMCData')\n",
    "\n",
    "# Load surveys\n",
    "with open('../Pickle/'+prefix+'surveys.pkl', 'rb') as infile:\n",
    "    surveys=pickle.load(infile)\n",
    "    names=pickle.load(infile)\n",
    "\n",
    "# Load grids\n",
    "with open('../Pickle/'+prefix+'grids.pkl', 'rb') as infile:\n",
    "    grids=pickle.load(infile)\n",
    "\n",
    "# Make params with the correct parameters but no priors\n",
    "params = {}\n",
    "for key in labels:\n",
    "    params[key] = {\n",
    "        \"min\": -np.inf,\n",
    "        \"max\": np.inf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_fit[\"lEmin\"] = 30\n",
    "fit = {}\n",
    "# best_fit[\"gamma\"] = -1.0\n",
    "newC, llc = it.minimise_const_only(best_fit, grids, surveys)\n",
    "\n",
    "for s,g in zip(surveys, grids):\n",
    "    g.state.FRBdemo.lC = newC\n",
    "\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    ax.set_title(s.name)\n",
    "    ax.set_xlabel(\"DM\")\n",
    "    ax.set_ylabel(\"P(DM)\")\n",
    "    ax.set_xlim(xmax=7000)\n",
    "\n",
    "    rates=g.rates\n",
    "    dmvals=g.dmvals\n",
    "    pdm=np.sum(rates,axis=0)\n",
    "\n",
    "    ax.plot(dmvals, pdm)\n",
    "\n",
    "    expected=it.CalculateIntegral(g,s)\n",
    "    expected *= 10**g.state.FRBdemo.lC\n",
    "    observed=s.NORM_FRB\n",
    "\n",
    "    print(s.name + \" - expected, observed: \" + str(expected) + \", \" + str(observed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uDMGs = 0.5\n",
    "\n",
    "fig = plt.figure(figsize=(10,40))\n",
    "\n",
    "for j,(s,g) in enumerate(zip(surveys, grids)):\n",
    "    ax = fig.add_subplot(len(surveys),1,j+1)\n",
    "    plt.title(s.name)\n",
    "    ax.set_xlabel('DM')\n",
    "    ax.set_ylabel('Weight')\n",
    "\n",
    "    rates=g.rates\n",
    "    dmvals=g.dmvals\n",
    "    zvals=g.zvals\n",
    "    DMobs=s.DMEGs\n",
    "\n",
    "    dm_weights, iweights = it.calc_DMG_weights(DMobs, s.DMGs, uDMGs, dmvals)\n",
    "    for i in range(len(DMobs)):\n",
    "        ax.plot(dmvals[iweights[i]], dm_weights[i], '.-', label=s.frbs[\"TNS\"][i] + \" \" + str(s.DMGs[i]))\n",
    "\n",
    "    ax.legend()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
